{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "import datetime as dt\n",
    "import matplotlib.dates as mdates\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file containing the power generation data (We drop ID's as we train on one plant)\n",
    "gen_1=pd.read_csv('/Users/flo/Development/BetterSolar/BetterSolarBetaV0Python/Data/Static/Plant_1_Generation_Data.csv')\n",
    "gen_1.drop('PLANT_ID',1,inplace=True)\n",
    "# Read the CSV file containing external data inputs (such as weather, temperature...)\n",
    "sens_1= pd.read_csv('/Users/flo/Development/BetterSolar/BetterSolarBetaV0Python/Data/Static/Plant_1_Weather_Sensor_Data.csv')\n",
    "sens_1.drop('PLANT_ID',1,inplace=True)\n",
    "#format datetime\n",
    "gen_1['DATE_TIME']= pd.to_datetime(gen_1['DATE_TIME'],format='%d-%m-%Y %H:%M')\n",
    "sens_1['DATE_TIME']= pd.to_datetime(sens_1['DATE_TIME'],format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format time format\n",
    "df_gen=gen_1.groupby('DATE_TIME').sum().reset_index()\n",
    "df_gen['time']=df_gen['DATE_TIME'].dt.time\n",
    "# Set up plot to inspect yields\n",
    "fig,ax = plt.subplots(ncols=2,nrows=1,dpi=100,figsize=(20,5))\n",
    "# daily yield plot\n",
    "df_gen.plot(x='DATE_TIME',y='DAILY_YIELD',color='navy',ax=ax[0])\n",
    "# AC & DC power plot\n",
    "df_gen.set_index('time').drop('DATE_TIME',1)[['AC_POWER','DC_POWER']].plot(style='o',ax=ax[1])\n",
    "\n",
    "ax[0].set_title('Daily Yield',)\n",
    "ax[1].set_title('AC power & DC power during day hours')\n",
    "ax[0].set_ylabel('kW',color='navy',fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_gen=df_gen.copy()\n",
    "daily_gen['date']=daily_gen['DATE_TIME'].dt.date\n",
    "\n",
    "daily_gen=daily_gen.groupby('date').sum()\n",
    "\n",
    "fig,ax= plt.subplots(ncols=2,dpi=100,figsize=(20,5))\n",
    "daily_gen['DAILY_YIELD'].plot(ax=ax[0],color='navy')\n",
    "daily_gen['TOTAL_YIELD'].plot(kind='bar',ax=ax[1],color='navy')\n",
    "fig.autofmt_xdate(rotation=45)\n",
    "ax[0].set_title('Daily Yield')\n",
    "ax[1].set_title('Total Yield')\n",
    "ax[0].set_ylabel('kW',color='navy',fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sens=sens_1.groupby('DATE_TIME').sum().reset_index()\n",
    "df_sens['time']=df_sens['DATE_TIME'].dt.time\n",
    "\n",
    "fig,ax = plt.subplots(ncols=2,nrows=1,dpi=100,figsize=(20,5))\n",
    "# daily yield plot\n",
    "df_sens.plot(x='time',y='IRRADIATION',ax=ax[0],style='o')\n",
    "# AC & DC power plot\n",
    "df_sens.set_index('DATE_TIME').drop('time',1)[['AMBIENT_TEMPERATURE','MODULE_TEMPERATURE']].plot(ax=ax[1])\n",
    "\n",
    "ax[0].set_title('Irradiation during day hours',)\n",
    "ax[1].set_title('Ambient and Module temperature')\n",
    "ax[0].set_ylabel('W/m',color='navy',fontsize=17)\n",
    "ax[1].set_ylabel('Â°C',color='navy',fontsize=17)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solar Irridation seems to not be affected by neither the module- nor the ambient temperature.\n",
    "This means power generation is not heavily dependent on the temperature.\n",
    "Falty equipment on the other hand could be impacted by temperature.\n",
    "\n",
    "We also need to inspect the power conversion.\n",
    "Basic PV plant: Sun -> DC Power -> conversion -> AC power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=gen_1.copy()\n",
    "losses['day']=losses['DATE_TIME'].dt.date\n",
    "losses=losses.groupby('day').sum()\n",
    "losses['losses']=losses['AC_POWER']/losses['DC_POWER']*100\n",
    "\n",
    "losses['losses'].plot(style='o--',figsize=(17,5),label='Real Power')\n",
    "\n",
    "plt.title('% of DC power converted in AC power',size=17)\n",
    "plt.ylabel('DC power converted (%)',fontsize=14,color='red')\n",
    "plt.axhline(losses['losses'].mean(),linestyle='--',color='gray',label='mean')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only about 9% of the power gets converted. THis means the inverter has a lot of room for optimisation. \n",
    "We have different inverters that yield different outputs. We need to determine which inverters underperform. To do so, we'll check the DC Output by source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources=gen_1.copy()\n",
    "sources['time']=sources['DATE_TIME'].dt.time\n",
    "sources.set_index('time').groupby('SOURCE_KEY')['DC_POWER'].plot(style='o',legend=True,figsize=(20,10))\n",
    "plt.title('DC Power during day for all sources',size=17)\n",
    "plt.ylabel('DC POWER ( kW )',color='navy',fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we see outliers in the data during the day, which might indicate faulty inverters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_gen=gen_1.copy()\n",
    "dc_gen['time']=dc_gen['DATE_TIME'].dt.time\n",
    "dc_gen=dc_gen.groupby(['time','SOURCE_KEY'])['DC_POWER'].mean().unstack()\n",
    "\n",
    "cmap = sns.color_palette(\"Spectral\", n_colors=12)\n",
    "\n",
    "fig,ax=plt.subplots(ncols=2,nrows=1,dpi=100,figsize=(20,6))\n",
    "dc_gen.iloc[:,0:11].plot(ax=ax[0],color=cmap)\n",
    "dc_gen.iloc[:,11:22].plot(ax=ax[1],color=cmap)\n",
    "\n",
    "ax[0].set_title('First 11 sources')\n",
    "ax[0].set_ylabel('DC POWER ( kW )',fontsize=17,color='navy')\n",
    "ax[1].set_title('Last 11 sources')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some inverters are underperforming.\n",
    "Why? Temperature? Weather?\n",
    "Let's plot the output against some external data and see if we can draw any conclusions from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to merge the dataframes in order to plot any sensor and generation data against each other.\n",
    "temp1_gen=gen_1.copy()\n",
    "\n",
    "temp1_gen['time']=temp1_gen['DATE_TIME'].dt.time\n",
    "temp1_gen['day']=temp1_gen['DATE_TIME'].dt.date\n",
    "\n",
    "\n",
    "temp1_sens=sens_1.copy()\n",
    "\n",
    "temp1_sens['time']=temp1_sens['DATE_TIME'].dt.time\n",
    "temp1_sens['day']=temp1_sens['DATE_TIME'].dt.date\n",
    "\n",
    "# just for columns\n",
    "cols=temp1_gen.groupby(['time','day'])['DC_POWER'].mean().unstack()\n",
    "\n",
    "# Generate Plots\n",
    "ax =temp1_gen.groupby(['time','day'])['DC_POWER'].mean().unstack().plot(sharex=True,subplots=True,layout=(17,2),figsize=(20,30))\n",
    "temp1_gen.groupby(['time','day'])['DAILY_YIELD'].mean().unstack().plot(sharex=True,subplots=True,layout=(17,2),figsize=(20,20),style='-.',ax=ax)\n",
    "\n",
    "i=0\n",
    "for a in range(len(ax)):\n",
    "    for b in range(len(ax[a])):\n",
    "        ax[a,b].set_title(cols.columns[i],size=15)\n",
    "        ax[a,b].legend(['DC_POWER','DAILY_YIELD'])\n",
    "        i=i+1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_1.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= temp1_sens.groupby(['time','day'])['MODULE_TEMPERATURE'].mean().unstack().plot(subplots=True,layout=(17,2),figsize=(20,30))\n",
    "temp1_sens.groupby(['time','day'])['AMBIENT_TEMPERATURE'].mean().unstack().plot(subplots=True,layout=(17,2),figsize=(20,40),style='-.',ax=ax)\n",
    "\n",
    "i=0\n",
    "for a in range(len(ax)):\n",
    "    for b in range(len(ax[a])):\n",
    "        ax[a,b].axhline(50)\n",
    "        ax[a,b].set_title(cols.columns[i],size=15)\n",
    "        ax[a,b].legend(['Module Temperature','Ambient Temperature'])\n",
    "        i=i+1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can determine which inverters cause problems, as we can see, \n",
    "the output comes to an halt during sunlight in some cases. \n",
    "We have to assume that this is not only a data recording error, but a faulty inverter.\n",
    "\n",
    "We need to further inspect the unusual behaviours of the inverter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_source=gen_1[gen_1['SOURCE_KEY']=='bvBOhCH3iADSZry']\n",
    "worst_source['time']=worst_source['DATE_TIME'].dt.time\n",
    "worst_source['day']=worst_source['DATE_TIME'].dt.date\n",
    "\n",
    "ax=worst_source.groupby(['time','day'])['DC_POWER'].mean().unstack().plot(sharex=True,subplots=True,layout=(17,2),figsize=(20,30))\n",
    "worst_source.groupby(['time','day'])['DAILY_YIELD'].mean().unstack().plot(sharex=True,subplots=True,layout=(17,2),figsize=(20,30),ax=ax,style='-.')\n",
    "\n",
    "i=0\n",
    "for a in range(len(ax)):\n",
    "    for b in range(len(ax[a])):\n",
    "        ax[a,b].set_title(cols.columns[i],size=15)\n",
    "        ax[a,b].legend(['DC_POWER','DAILY_YIELD'])\n",
    "        i=i+1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This far, we reviewed the data and tried to find faulty inverters.\n",
    "\n",
    "Let's get to prediction - but first - we are going to test the null hypothesis.\n",
    "We do this to see if data is stationary.\n",
    "Remember, if the p value is smaller than 0,1 (some use 0,05), we conclude that we deal with a non stationary problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import DateOffset\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "pred_gen=gen_1.copy()\n",
    "pred_gen=pred_gen.groupby('DATE_TIME').sum()\n",
    "pred_gen=pred_gen['DAILY_YIELD'][-288:].reset_index()\n",
    "pred_gen.set_index('DATE_TIME',inplace=True)\n",
    "pred_gen.head()\n",
    "\n",
    "\n",
    "result = adfuller(pred_gen['DAILY_YIELD'])\n",
    "print('Augmented Dickey-Fuller Test:')\n",
    "labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n",
    "\n",
    "for value,label in zip(result,labels):\n",
    "    print(label+' : '+str(value) )\n",
    "    \n",
    "if result[1] <= 0.05:\n",
    "    print(\"Evidence against Null Hypothesis. Data has no unit root and is stationary\")\n",
    "else:\n",
    "    print(\"Virtually no evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we have used the Augmented Dickey-Fuller unit root test and concluded\n",
    "that the data is non-stationary. \n",
    "Let's do the standart data splitting for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pred_gen[:192]\n",
    "test=pred_gen[-96:]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(train,label='Train',color='navy')\n",
    "plt.plot(test,label='Test',color='darkorange')\n",
    "plt.title('Last 4 days of daily yield',fontsize=17)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Testing\n",
    "# Data Engineering\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "dfs = [sens_1, temp1_gen]\n",
    "# Merge Dataframes to train DL model\n",
    "dfFinal = reduce(lambda left, right: pd.merge(left,right,on=[\"DATE_TIME\"], how='outer'),dfs)\n",
    "dfFinal = finalDf.drop(\"SOURCE_KEY_x\",axis='columns').drop(\"time\",axis='columns').drop(\"day\",axis='columns').drop(\"SOURCE_KEY_y\", axis='columns')\n",
    "\n",
    "# Take every fourth record to predict hours rather than 15 min intervals\n",
    "dfFinal = dfFinal[5::4]\n",
    "\n",
    "date_time = pd.to_datetime(dfFinal.pop('DATE_TIME'), format='%Y.%m.%d %H:%M:%S')\n",
    "# Normalize date time\n",
    "timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "dfFinal['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "dfFinal['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "dfFinal['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "dfFinal['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "\n",
    "dfFinal = dfFinal.fillna(value=0)\n",
    "column_indices = {name: i for i, name in enumerate(dfFinal.columns)}\n",
    "n = len(dfFinal)\n",
    "train_df = dfFinal[0:int(n*0.7)]\n",
    "val_df = dfFinal[int(n*0.7):int(n*0.9)]\n",
    "test_df = dfFinal[int(n*0.9):]\n",
    "\n",
    "\n",
    "# # Normalize data\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std\n",
    "\n",
    "dfFinal.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect feature distribution\n",
    "\n",
    "df_std = (dfFinal - train_mean) / train_std\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "_ = ax.set_xticklabels(dfFinal.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did all the necessary feature engineering to normalize the data, eliminate string and date values and split into Training, Test and CV data.\n",
    "\n",
    "We now need to define the data windowing.\n",
    "The main features of the input windows are:\n",
    "\n",
    "The width (number of time steps) of the input and label windows.\n",
    "The time offset between them.\n",
    "Which features are used as inputs, labels, or both.\n",
    "\n",
    "For example: \n",
    "Input width = 24\n",
    "offset = 24\n",
    "total width = 48\n",
    "label width = 1\n",
    "\n",
    "=\n",
    "\n",
    "make a single prediction 24 hours into the future, given 24 hours of history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our example\n",
    "w1 = WindowGenerator(input_width=24, label_width=1, shift=24,\n",
    "                     label_columns=['TOTAL_YIELD'])\n",
    "                     \n",
    "w1                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besides the data windowing we also need split windows\n",
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "example_window = tf.stack([np.array(train_df[:w1.total_window_size]),\n",
    "                           np.array(train_df[100:100+w1.total_window_size]),\n",
    "                           np.array(train_df[200:200+w1.total_window_size])])\n",
    "\n",
    "example_inputs, example_labels = w1.split_window(example_window)\n",
    "\n",
    "print('All shapes are: (batch, time, features)')\n",
    "print(f'Window shape: {example_window.shape}')\n",
    "print(f'Inputs shape: {example_inputs.shape}')\n",
    "print(f'Labels shape: {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have engineered our data properly. We can finally start using Tensorflow by creating TF Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(self, data):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "  ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=True,\n",
    "      batch_size=32,)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset\n",
    "\n",
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset engineering was successful\n",
    "for example_inputs, example_labels in w1.train.take(1):\n",
    "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a simple single step model with linear regression\n",
    "\n",
    "single_step_window = WindowGenerator(\n",
    "    input_width=1, label_width=1, shift=1,\n",
    "    label_columns=['TOTAL_YIELD'])\n",
    "single_step_window\n",
    "\n",
    "for example_inputs, example_labels in single_step_window.train.take(1):\n",
    "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch, time, features): {example_labels.shape}')\n",
    "\n",
    "\n",
    "###################\n",
    "# Baseline Model #\n",
    "###################\n",
    "class Baseline(tf.keras.Model):\n",
    "  def __init__(self, label_index=None):\n",
    "    super().__init__()\n",
    "    self.label_index = label_index\n",
    "\n",
    "  def call(self, inputs):\n",
    "    if self.label_index is None:\n",
    "      return inputs\n",
    "    result = inputs[:, :, self.label_index]\n",
    "    return result[:, :, tf.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(label_index=column_indices['TOTAL_YIELD'])\n",
    "\n",
    "baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "val_performance['Baseline'] = baseline.evaluate(single_step_window.val)\n",
    "performance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)\n",
    "\n",
    "wide_window = WindowGenerator(\n",
    "    input_width=24, label_width=24, shift=1,\n",
    "    label_columns=['TOTAL_YIELD'])\n",
    "\n",
    "wide_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Modelling\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "print('Input shape:', single_step_window.example[0].shape)\n",
    "print('Output shape:', dense(single_step_window.example[0]).shape)\n",
    "\n",
    "MAX_EPOCHS = 10\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "  model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can train the simple linear model and evaluate the performance\n",
    "\n",
    "history = compile_and_fit(dense, single_step_window)\n",
    "\n",
    "val_performance['Linear'] = dense.evaluate(single_step_window.val)\n",
    "performance['Linear'] = dense.evaluate(single_step_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad performance due to single step model\n",
    "# Need to train a multi time step model\n",
    "import IPython\n",
    "CONV_WIDTH = 3\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=CONV_WIDTH,\n",
    "    label_width=1,\n",
    "    shift=1,\n",
    "    label_columns=['TOTAL_YIELD'])\n",
    "\n",
    "multi_step_dense = tf.keras.Sequential([\n",
    "    # Shape: (time, features) => (time*features)\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "    # Add back the time dimension.\n",
    "    # Shape: (outputs) => (1, outputs)\n",
    "    tf.keras.layers.Reshape([1, -1]),\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_step_dense, conv_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.val)\n",
    "performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better performance but still not satisfactory\n",
    "# Not only performance is bad, but the input shape is fixed\n",
    "# One way to tackle these problems is using CNN'sens_1\n",
    "conv_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,\n",
    "                           kernel_size=(CONV_WIDTH,),\n",
    "                           activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "])\n",
    "\n",
    "history = compile_and_fit(conv_model, conv_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Conv'] = conv_model.evaluate(conv_window.val)\n",
    "performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_WIDTH = 24\n",
    "INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)\n",
    "wide_conv_window = WindowGenerator(\n",
    "    input_width=INPUT_WIDTH,\n",
    "    label_width=LABEL_WIDTH,\n",
    "    shift=1,\n",
    "    label_columns=['TOTAL_YIELD'])\n",
    "\n",
    "wide_conv_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Models\n",
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "history = compile_and_fit(lstm_model, wide_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\n",
    "performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Evaluation\n",
    "\n",
    "x = np.arange(len(performance))\n",
    "width = 0.3\n",
    "metric_name = 'mean_absolute_error'\n",
    "metric_index = lstm_model.metrics_names.index('mean_absolute_error')\n",
    "val_mae = [v[metric_index] for v in val_performance.values()]\n",
    "test_mae = [v[metric_index] for v in performance.values()]\n",
    "\n",
    "plt.ylabel('mean_absolute_error [TOTAL_YIELD, normalized]')\n",
    "plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
    "plt.bar(x + 0.17, test_mae, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=performance.keys(),\n",
    "           rotation=45)\n",
    "_ = plt.legend()\n",
    "\n",
    "for name, value in performance.items():\n",
    "  print(f'{name:12s}: {value[1]:0.4f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
